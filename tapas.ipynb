{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb24991",
   "metadata": {},
   "source": [
    "# TAPAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f6bf3",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/model_doc/tapas <br>\n",
    "https://paperswithcode.com/method/tapas <br>\n",
    "https://arxiv.org/abs/2004.02349v2 <br>\n",
    "https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r transformers\n",
    "! git clone https://github.com/huggingface/transformers.git\n",
    "! cd transformers\n",
    "! pip install ./transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d83ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33952e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "import os\n",
    "\n",
    "def download_files(dir_name):\n",
    "    if not os.path.exists(dir_name): \n",
    "        # 28 training examples from the SQA training set + table csv data\n",
    "        urls = [\"https://www.dropbox.com/s/2p6ez9xro357i63/sqa_train_set_28_examples.zip?dl=1\",\n",
    "                \"https://www.dropbox.com/s/abhum8ssuow87h6/table_csv.zip?dl=1\"\n",
    "        ]\n",
    "        for url in urls:\n",
    "            r = requests.get(url)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall()\n",
    "\n",
    "dir_name = \"sqa_data\"\n",
    "download_files(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6f31e1",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1416ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n"
     ]
    }
   ],
   "source": [
    "! pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cf3a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>position</th>\n",
       "      <th>question</th>\n",
       "      <th>table_file</th>\n",
       "      <th>answer_coordinates</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>where are the players from?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>['(0, 4)', '(1, 4)', '(2, 4)', '(3, 4)', '(4, ...</td>\n",
       "      <td>['Louisiana State University', 'Valley HS (Las...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>which player went to louisiana state university?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>['(0, 1)']</td>\n",
       "      <td>['Ben McDonald']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>who are the players?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ...</td>\n",
       "      <td>['Ben McDonald', 'Tyler Houston', 'Roger Salke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>which ones are in the top 26 picks?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ...</td>\n",
       "      <td>['Ben McDonald', 'Tyler Houston', 'Roger Salke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>and of those, who is from louisiana state univ...</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>['(0, 1)']</td>\n",
       "      <td>['Ben McDonald']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  annotator  position  \\\n",
       "0  nt-639          0         0   \n",
       "1  nt-639          0         1   \n",
       "2  nt-639          1         0   \n",
       "3  nt-639          1         1   \n",
       "4  nt-639          1         2   \n",
       "\n",
       "                                            question             table_file  \\\n",
       "0                        where are the players from?  table_csv/203_149.csv   \n",
       "1   which player went to louisiana state university?  table_csv/203_149.csv   \n",
       "2                               who are the players?  table_csv/203_149.csv   \n",
       "3                which ones are in the top 26 picks?  table_csv/203_149.csv   \n",
       "4  and of those, who is from louisiana state univ...  table_csv/203_149.csv   \n",
       "\n",
       "                                  answer_coordinates  \\\n",
       "0  ['(0, 4)', '(1, 4)', '(2, 4)', '(3, 4)', '(4, ...   \n",
       "1                                         ['(0, 1)']   \n",
       "2  ['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ...   \n",
       "3  ['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ...   \n",
       "4                                         ['(0, 1)']   \n",
       "\n",
       "                                         answer_text  \n",
       "0  ['Louisiana State University', 'Valley HS (Las...  \n",
       "1                                   ['Ben McDonald']  \n",
       "2  ['Ben McDonald', 'Tyler Houston', 'Roger Salke...  \n",
       "3  ['Ben McDonald', 'Tyler Houston', 'Roger Salke...  \n",
       "4                                   ['Ben McDonald']  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data = pd.read_excel(\"sqa_train_set_28_examples.xlsx\")\n",
    "train_data = pd.read_csv('sqa_data/train.tsv', sep='\\t')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27417c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'annotator', 'position', 'question', 'table_file',\n",
       "       'answer_coordinates', 'answer_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1ca67eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14541"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56296bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def _parse_answer_coordinates(answer_coordinate_str):\n",
    "    \"\"\"\n",
    "    Parses the answer_coordinates of a question.\n",
    "    Args:\n",
    "    answer_coordinate_str: A string representation of a Python list of tuple\n",
    "      strings.\n",
    "      For example: \"['(1, 4)','(1, 3)', ...]\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        answer_coordinates = []\n",
    "        # make a list of strings\n",
    "        coords = ast.literal_eval(answer_coordinate_str)\n",
    "        # parse each string as a tuple\n",
    "        for row_index, column_index in sorted(\n",
    "            ast.literal_eval(coord) for coord in coords):\n",
    "            answer_coordinates.append((row_index, column_index))\n",
    "    except SyntaxError:\n",
    "        raise ValueError('Unable to evaluate %s' % answer_coordinate_str)\n",
    "  \n",
    "    return answer_coordinates\n",
    "\n",
    "\n",
    "def _parse_answer_text(answer_text):\n",
    "    \"\"\"\n",
    "    Populates the answer_texts field of `answer` by parsing `answer_text`.\n",
    "    Args:\n",
    "    answer_text: A string representation of a Python list of strings.\n",
    "      For example: \"[u'test', u'hello', ...]\"\n",
    "    answer: an Answer object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        answer = []\n",
    "        for value in ast.literal_eval(answer_text):\n",
    "            answer.append(value)\n",
    "    except SyntaxError:\n",
    "        raise ValueError('Unable to evaluate %s' % answer_text)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa9262d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>position</th>\n",
       "      <th>question</th>\n",
       "      <th>table_file</th>\n",
       "      <th>answer_coordinates</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>where are the players from?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...</td>\n",
       "      <td>[Louisiana State University, Valley HS (Las Ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>which player went to louisiana state university?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[Ben McDonald]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>who are the players?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>which ones are in the top 26 picks?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>and of those, who is from louisiana state univ...</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[Ben McDonald]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>who are the players in the top 26?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>of those, which one was from louisiana state u...</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[Ben McDonald]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nt-11649</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>what are all the names of the teams?</td>\n",
       "      <td>table_csv/204_135.csv</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[Cordoba CF, CD Malaga, Granada CF, UD Las Pal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nt-11649</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>of these, which teams had any losses?</td>\n",
       "      <td>table_csv/204_135.csv</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[Cordoba CF, CD Malaga, Granada CF, UD Las Pal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nt-11649</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>of these teams, which had more than 21 losses?</td>\n",
       "      <td>table_csv/204_135.csv</td>\n",
       "      <td>[(15, 1)]</td>\n",
       "      <td>[CD Villarrobledo]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  annotator  position  \\\n",
       "0    nt-639          0         0   \n",
       "1    nt-639          0         1   \n",
       "2    nt-639          1         0   \n",
       "3    nt-639          1         1   \n",
       "4    nt-639          1         2   \n",
       "5    nt-639          2         0   \n",
       "6    nt-639          2         1   \n",
       "7  nt-11649          0         0   \n",
       "8  nt-11649          0         1   \n",
       "9  nt-11649          0         2   \n",
       "\n",
       "                                            question             table_file  \\\n",
       "0                        where are the players from?  table_csv/203_149.csv   \n",
       "1   which player went to louisiana state university?  table_csv/203_149.csv   \n",
       "2                               who are the players?  table_csv/203_149.csv   \n",
       "3                which ones are in the top 26 picks?  table_csv/203_149.csv   \n",
       "4  and of those, who is from louisiana state univ...  table_csv/203_149.csv   \n",
       "5                 who are the players in the top 26?  table_csv/203_149.csv   \n",
       "6  of those, which one was from louisiana state u...  table_csv/203_149.csv   \n",
       "7               what are all the names of the teams?  table_csv/204_135.csv   \n",
       "8              of these, which teams had any losses?  table_csv/204_135.csv   \n",
       "9     of these teams, which had more than 21 losses?  table_csv/204_135.csv   \n",
       "\n",
       "                                  answer_coordinates  \\\n",
       "0  [(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...   \n",
       "1                                           [(0, 1)]   \n",
       "2  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "3  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "4                                           [(0, 1)]   \n",
       "5  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "6                                           [(0, 1)]   \n",
       "7  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "8  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "9                                          [(15, 1)]   \n",
       "\n",
       "                                         answer_text  \n",
       "0  [Louisiana State University, Valley HS (Las Ve...  \n",
       "1                                     [Ben McDonald]  \n",
       "2  [Ben McDonald, Tyler Houston, Roger Salkeld, J...  \n",
       "3  [Ben McDonald, Tyler Houston, Roger Salkeld, J...  \n",
       "4                                     [Ben McDonald]  \n",
       "5  [Ben McDonald, Tyler Houston, Roger Salkeld, J...  \n",
       "6                                     [Ben McDonald]  \n",
       "7  [Cordoba CF, CD Malaga, Granada CF, UD Las Pal...  \n",
       "8  [Cordoba CF, CD Malaga, Granada CF, UD Las Pal...  \n",
       "9                                 [CD Villarrobledo]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['answer_coordinates'] = train_data['answer_coordinates'].apply(lambda coords_str: _parse_answer_coordinates(coords_str))\n",
    "train_data['answer_text'] = train_data['answer_text'].apply(lambda txt: _parse_answer_text(txt))\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33a4c794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>position</th>\n",
       "      <th>question</th>\n",
       "      <th>table_file</th>\n",
       "      <th>answer_coordinates</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>sequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>where are the players from?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...</td>\n",
       "      <td>[Louisiana State University, Valley HS (Las Ve...</td>\n",
       "      <td>nt-639-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>which player went to louisiana state university?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[Ben McDonald]</td>\n",
       "      <td>nt-639-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>who are the players?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n",
       "      <td>nt-639-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>which ones are in the top 26 picks?</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n",
       "      <td>nt-639-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nt-639</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>and of those, who is from louisiana state univ...</td>\n",
       "      <td>table_csv/203_149.csv</td>\n",
       "      <td>[(0, 1)]</td>\n",
       "      <td>[Ben McDonald]</td>\n",
       "      <td>nt-639-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  annotator  position  \\\n",
       "0  nt-639          0         0   \n",
       "1  nt-639          0         1   \n",
       "2  nt-639          1         0   \n",
       "3  nt-639          1         1   \n",
       "4  nt-639          1         2   \n",
       "\n",
       "                                            question             table_file  \\\n",
       "0                        where are the players from?  table_csv/203_149.csv   \n",
       "1   which player went to louisiana state university?  table_csv/203_149.csv   \n",
       "2                               who are the players?  table_csv/203_149.csv   \n",
       "3                which ones are in the top 26 picks?  table_csv/203_149.csv   \n",
       "4  and of those, who is from louisiana state univ...  table_csv/203_149.csv   \n",
       "\n",
       "                                  answer_coordinates  \\\n",
       "0  [(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...   \n",
       "1                                           [(0, 1)]   \n",
       "2  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "3  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n",
       "4                                           [(0, 1)]   \n",
       "\n",
       "                                         answer_text sequence_id  \n",
       "0  [Louisiana State University, Valley HS (Las Ve...    nt-639-0  \n",
       "1                                     [Ben McDonald]    nt-639-0  \n",
       "2  [Ben McDonald, Tyler Houston, Roger Salkeld, J...    nt-639-1  \n",
       "3  [Ben McDonald, Tyler Houston, Roger Salkeld, J...    nt-639-1  \n",
       "4                                     [Ben McDonald]    nt-639-1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sequence_id(example_id, annotator):\n",
    "    if \"-\" in str(annotator):\n",
    "        raise ValueError('\"-\" not allowed in annotator.')\n",
    "    return f\"{example_id}-{annotator}\"\n",
    "\n",
    "train_data['sequence_id'] = train_data.apply(lambda x: get_sequence_id(x.id, x.annotator), axis=1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c5d43b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>table_file</th>\n",
       "      <th>answer_coordinates</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ns-1006-0</th>\n",
       "      <td>[what are the game titles?, which of the games...</td>\n",
       "      <td>table_csv/203_583.csv</td>\n",
       "      <td>[[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, ...</td>\n",
       "      <td>[[Buggy Grand Prix: Kattobi! Dai-Sakusen, Gunb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ns-1006-1</th>\n",
       "      <td>[what are the psikyo titles?, which of these h...</td>\n",
       "      <td>table_csv/203_583.csv</td>\n",
       "      <td>[[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, ...</td>\n",
       "      <td>[[Buggy Grand Prix: Kattobi! Dai-Sakusen, Gunb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ns-1006-2</th>\n",
       "      <td>[what are the notes for the listed games?, whi...</td>\n",
       "      <td>table_csv/203_583.csv</td>\n",
       "      <td>[[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, ...</td>\n",
       "      <td>[[, , Released and published in Europe by Play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ns-1013-0</th>\n",
       "      <td>[which countries received more than 10 medals?...</td>\n",
       "      <td>table_csv/204_922.csv</td>\n",
       "      <td>[[(1, 1), (2, 1), (3, 1)], [(1, 1)]]</td>\n",
       "      <td>[[Colombia, Dominican Republic, Peru], [Colomb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ns-1013-1</th>\n",
       "      <td>[which countries were in taekwondo at the 2013...</td>\n",
       "      <td>table_csv/204_922.csv</td>\n",
       "      <td>[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...</td>\n",
       "      <td>[[Venezuela, Colombia, Dominican Republic, Per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ns-1013-2</th>\n",
       "      <td>[what are the total number of medals each coun...</td>\n",
       "      <td>table_csv/204_922.csv</td>\n",
       "      <td>[[(0, 5), (1, 5), (2, 5), (3, 5), (4, 5), (5, ...</td>\n",
       "      <td>[[10, 17, 11, 14, 8, 3, 5, 3, 1, 1], [17], [Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ns-1035-0</th>\n",
       "      <td>[who are all of the drivers?, how many points ...</td>\n",
       "      <td>table_csv/204_641.csv</td>\n",
       "      <td>[[(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, ...</td>\n",
       "      <td>[[Jim Clark, Denny Hulme, Chris Amon, Jack Bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ns-1035-1</th>\n",
       "      <td>[who where the drivers at the 1967 british gra...</td>\n",
       "      <td>table_csv/204_641.csv</td>\n",
       "      <td>[[(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, ...</td>\n",
       "      <td>[[Jim Clark, Denny Hulme, Chris Amon, Jack Bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ns-1035-2</th>\n",
       "      <td>[who are all of the drivers?, how many points ...</td>\n",
       "      <td>table_csv/204_641.csv</td>\n",
       "      <td>[[(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, ...</td>\n",
       "      <td>[[Jim Clark, Denny Hulme, Chris Amon, Jack Bra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ns-1042-0</th>\n",
       "      <td>[what are the land areas?, of those, which are...</td>\n",
       "      <td>table_csv/203_459.csv</td>\n",
       "      <td>[[(0, 5), (1, 5), (2, 5), (3, 5), (4, 5), (5, ...</td>\n",
       "      <td>[[155 (60), 144 (55), 144 (56), 142 (55), 142 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      question  \\\n",
       "sequence_id                                                      \n",
       "ns-1006-0    [what are the game titles?, which of the games...   \n",
       "ns-1006-1    [what are the psikyo titles?, which of these h...   \n",
       "ns-1006-2    [what are the notes for the listed games?, whi...   \n",
       "ns-1013-0    [which countries received more than 10 medals?...   \n",
       "ns-1013-1    [which countries were in taekwondo at the 2013...   \n",
       "ns-1013-2    [what are the total number of medals each coun...   \n",
       "ns-1035-0    [who are all of the drivers?, how many points ...   \n",
       "ns-1035-1    [who where the drivers at the 1967 british gra...   \n",
       "ns-1035-2    [who are all of the drivers?, how many points ...   \n",
       "ns-1042-0    [what are the land areas?, of those, which are...   \n",
       "\n",
       "                        table_file  \\\n",
       "sequence_id                          \n",
       "ns-1006-0    table_csv/203_583.csv   \n",
       "ns-1006-1    table_csv/203_583.csv   \n",
       "ns-1006-2    table_csv/203_583.csv   \n",
       "ns-1013-0    table_csv/204_922.csv   \n",
       "ns-1013-1    table_csv/204_922.csv   \n",
       "ns-1013-2    table_csv/204_922.csv   \n",
       "ns-1035-0    table_csv/204_641.csv   \n",
       "ns-1035-1    table_csv/204_641.csv   \n",
       "ns-1035-2    table_csv/204_641.csv   \n",
       "ns-1042-0    table_csv/203_459.csv   \n",
       "\n",
       "                                            answer_coordinates  \\\n",
       "sequence_id                                                      \n",
       "ns-1006-0    [[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, ...   \n",
       "ns-1006-1    [[(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, ...   \n",
       "ns-1006-2    [[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, ...   \n",
       "ns-1013-0                 [[(1, 1), (2, 1), (3, 1)], [(1, 1)]]   \n",
       "ns-1013-1    [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...   \n",
       "ns-1013-2    [[(0, 5), (1, 5), (2, 5), (3, 5), (4, 5), (5, ...   \n",
       "ns-1035-0    [[(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, ...   \n",
       "ns-1035-1    [[(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, ...   \n",
       "ns-1035-2    [[(0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, ...   \n",
       "ns-1042-0    [[(0, 5), (1, 5), (2, 5), (3, 5), (4, 5), (5, ...   \n",
       "\n",
       "                                                   answer_text  \n",
       "sequence_id                                                     \n",
       "ns-1006-0    [[Buggy Grand Prix: Kattobi! Dai-Sakusen, Gunb...  \n",
       "ns-1006-1    [[Buggy Grand Prix: Kattobi! Dai-Sakusen, Gunb...  \n",
       "ns-1006-2    [[, , Released and published in Europe by Play...  \n",
       "ns-1013-0    [[Colombia, Dominican Republic, Peru], [Colomb...  \n",
       "ns-1013-1    [[Venezuela, Colombia, Dominican Republic, Per...  \n",
       "ns-1013-2    [[10, 17, 11, 14, 8, 3, 5, 3, 1, 1], [17], [Co...  \n",
       "ns-1035-0    [[Jim Clark, Denny Hulme, Chris Amon, Jack Bra...  \n",
       "ns-1035-1    [[Jim Clark, Denny Hulme, Chris Amon, Jack Bra...  \n",
       "ns-1035-2    [[Jim Clark, Denny Hulme, Chris Amon, Jack Bra...  \n",
       "ns-1042-0    [[155 (60), 144 (55), 144 (56), 142 (55), 142 ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's group table-question pairs by sequence id, and remove some columns we don't need \n",
    "grouped = train_data.groupby(by='sequence_id').agg(lambda x: x.tolist())\n",
    "grouped = grouped.drop(columns=['id', 'annotator', 'position'])\n",
    "grouped['table_file'] = grouped['table_file'].apply(lambda x: x[0])\n",
    "grouped.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebdb2434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Release</th>\n",
       "      <th>6th Gen</th>\n",
       "      <th>Handheld</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buggy Grand Prix: Kattobi! Dai-Sakusen</td>\n",
       "      <td>2003</td>\n",
       "      <td>PlayStation 2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gunbird Special Edition / Gunbird 1&amp;2</td>\n",
       "      <td>2004</td>\n",
       "      <td>PlayStation 2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Psikyo Shooting Collection Vol. 1: Strikers 19...</td>\n",
       "      <td>2004</td>\n",
       "      <td>PlayStation 2</td>\n",
       "      <td>nan</td>\n",
       "      <td>Released and published in Europe by Play It as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Psikyo Shooting Collection Vol. 2: Sengoku Ace...</td>\n",
       "      <td>2004</td>\n",
       "      <td>PlayStation 2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psikyo Shooting Collection Vol. 3: Sol Divide ...</td>\n",
       "      <td>2004</td>\n",
       "      <td>PlayStation 2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Taisen Hot Gimmick: Cosplay Mahjong</td>\n",
       "      <td>2004</td>\n",
       "      <td>PlayStation 2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sengoku Cannon</td>\n",
       "      <td>2005</td>\n",
       "      <td>nan</td>\n",
       "      <td>PSP</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Taisen Hot Gimmick: Axes-Jong</td>\n",
       "      <td>2005</td>\n",
       "      <td>PlayStation 2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Release        6th Gen  \\\n",
       "0             Buggy Grand Prix: Kattobi! Dai-Sakusen    2003  PlayStation 2   \n",
       "1              Gunbird Special Edition / Gunbird 1&2    2004  PlayStation 2   \n",
       "2  Psikyo Shooting Collection Vol. 1: Strikers 19...    2004  PlayStation 2   \n",
       "3  Psikyo Shooting Collection Vol. 2: Sengoku Ace...    2004  PlayStation 2   \n",
       "4  Psikyo Shooting Collection Vol. 3: Sol Divide ...    2004  PlayStation 2   \n",
       "5                Taisen Hot Gimmick: Cosplay Mahjong    2004  PlayStation 2   \n",
       "6                                     Sengoku Cannon    2005            nan   \n",
       "7                      Taisen Hot Gimmick: Axes-Jong    2005  PlayStation 2   \n",
       "\n",
       "  Handheld                                               Note  \n",
       "0      nan                                                nan  \n",
       "1      nan                                                nan  \n",
       "2      nan  Released and published in Europe by Play It as...  \n",
       "3      nan                                                nan  \n",
       "4      nan                                                nan  \n",
       "5      nan                                                nan  \n",
       "6      PSP                                                nan  \n",
       "7      nan                                                nan  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['what are the game titles?', 'which of the games have notes?']\n"
     ]
    }
   ],
   "source": [
    "# path to the directory containing all csv files\n",
    "table_csv_path = \"table_csv\"\n",
    "\n",
    "item = grouped.iloc[0]\n",
    "table = pd.read_csv(table_csv_path + item.table_file[9:]).astype(str) \n",
    "\n",
    "display(table)\n",
    "print(\"\")\n",
    "print(item.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f1b49",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a4728f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TapasTokenizer\n",
    "\n",
    "# initialize the tokenizer\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b719f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'numeric_values', 'numeric_values_scale', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(table=table, queries=item.question, answer_coordinates=item.answer_coordinates, answer_text=item.answer_text,\n",
    "                     truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fabcde92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] what are the game titles? [SEP] title release 6th gen handheld note buggy grand prix : kattobi! dai - sakusen 2003 playstation 2 [EMPTY] [EMPTY] gunbird special edition / gunbird 1 & 2 2004 playstation 2 [EMPTY] [EMPTY] psikyo shooting collection vol. 1 : strikers 1945 i & ii 2004 playstation 2 [EMPTY] released and published in europe by play it as 1945 i & ii : the arcade games. psikyo shooting collection vol. 2 : sengoku ace & sengoku blade 2004 playstation 2 [EMPTY] [EMPTY] psikyo shooting collection vol. 3 : sol divide & dragon blaze 2004 playstation 2 [EMPTY] [EMPTY] taisen hot gimmick : cosplay mahjong 2004 playstation 2 [EMPTY] [EMPTY] sengoku cannon 2005 [EMPTY] psp [EMPTY] taisen hot gimmick : axes - jong 2005 playstation 2 [EMPTY] [EMPTY] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "135ebeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert encoding[\"token_type_ids\"].shape == (3, 512, 7)\n",
    "assert encoding[\"token_type_ids\"][0][:,3].sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5851b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Buggy Grand Prix: Kattobi! Dai-Sakusen', 'Gunbird Special Edition / Gunbird 1&2', 'Psikyo Shooting Collection Vol. 1: Strikers 1945 I & II', 'Psikyo Shooting Collection Vol. 2: Sengoku Ace & Sengoku Blade', 'Psikyo Shooting Collection Vol. 3: Sol Divide & Dragon Blaze', 'Taisen Hot Gimmick: Cosplay Mahjong', 'Sengoku Cannon', 'Taisen Hot Gimmick: Axes-Jong']\n"
     ]
    }
   ],
   "source": [
    "print(item.answer_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4621653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, prev_label in zip (encoding[\"input_ids\"][1], encoding[\"token_type_ids\"][1][:,3]):\n",
    "#     if id != 0: # we skip padding tokens\n",
    "#         print(tokenizer.decode([id]), prev_label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1b392",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfe3efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        table = pd.read_csv(table_csv_path + item.table_file[9:]).astype(str) # TapasTokenizer expects the table data to be text only\n",
    "        if item.position != 0:\n",
    "          # use the previous table-question pair to correctly set the prev_labels token type ids\n",
    "            previous_item = self.df.iloc[idx-1]\n",
    "            encoding = self.tokenizer(table=table, \n",
    "                                    queries=[previous_item.question, item.question], \n",
    "                                    answer_coordinates=[previous_item.answer_coordinates, item.answer_coordinates], \n",
    "                                    answer_text=[previous_item.answer_text, item.answer_text],\n",
    "                                    padding=\"max_length\",\n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\"\n",
    "            )\n",
    "            # use encodings of second table-question pair in the batch\n",
    "            encoding = {key: val[-1] for key, val in encoding.items()}\n",
    "        else:\n",
    "            # this means it's the first table-question pair in a sequence\n",
    "            encoding = self.tokenizer(table=table, \n",
    "                                    queries=item.question, \n",
    "                                    answer_coordinates=item.answer_coordinates, \n",
    "                                    answer_text=item.answer_text,\n",
    "                                    padding=\"max_length\",\n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\"\n",
    "            )\n",
    "            # remove the batch dimension which the tokenizer adds \n",
    "            encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        return encoding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e7d6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TableDataset(df=train_data, tokenizer=tokenizer)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f96ec42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 7])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"token_type_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "213d762b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9fe2e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f11247f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a0742a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 7])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"token_type_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0d93a",
   "metadata": {},
   "source": [
    "### Decoding & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ade5f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] where are the players from? [SEP] pick player team position school 1 ben mcdonald baltimore orioles rhp louisiana state university 2 tyler houston atlanta braves c valley hs ( las vegas, nv ) 3 roger salkeld seattle mariners rhp saugus ( ca ) hs 4 jeff jackson philadelphia phillies of simeon hs ( chicago, il ) 5 donald harris texas rangers of texas tech university 6 paul coleman saint louis cardinals of frankston ( tx ) hs 7 frank thomas chicago white sox 1b auburn university 8 earl cunningham chicago cubs of lancaster ( sc ) hs 9 kyle abbott california angels lhp long beach state university 10 charles johnson montreal expos c westwood hs ( fort pierce, fl ) 11 calvin murray cleveland indians 3b w. t. white high school ( dallas, tx ) 12 jeff juden houston astros rhp salem ( ma ) hs 13 brent mayne kansas city royals c cal state fullerton 14 steve hosey san francisco giants of fresno state university 15 kiki jones los angeles dodgers rhp hillsborough hs ( tampa, fl ) 16 greg blosser boston red sox of sarasota ( fl ) hs 17 cal eldred milwaukee brewers rhp university of iowa 18 willie greene pittsburgh pirates ss jones county hs ( gray, ga ) 19 eddie zosky toronto blue jays ss fresno state university 20 scott bryant cincinnati reds of university of texas 21 greg gohr detroit tigers rhp santa clara university 22 tom goodwin los angeles dodgers of fresno state university 23 mo vaughn boston red sox 1b seton hall university 24 alan zinter new york mets c university of arizona 25 chuck knoblauch minnesota twins 2b texas a & m university 26 scott burrell seattle mariners rhp hamden ( ct ) hs [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cb02acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first example should not have any prev_labels set\n",
    "assert batch[\"token_type_ids\"][0][:,3].sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9711298b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] which player went to louisiana state university? [SEP] pick player team position school 1 ben mcdonald baltimore orioles rhp louisiana state university 2 tyler houston atlanta braves c valley hs ( las vegas, nv ) 3 roger salkeld seattle mariners rhp saugus ( ca ) hs 4 jeff jackson philadelphia phillies of simeon hs ( chicago, il ) 5 donald harris texas rangers of texas tech university 6 paul coleman saint louis cardinals of frankston ( tx ) hs 7 frank thomas chicago white sox 1b auburn university 8 earl cunningham chicago cubs of lancaster ( sc ) hs 9 kyle abbott california angels lhp long beach state university 10 charles johnson montreal expos c westwood hs ( fort pierce, fl ) 11 calvin murray cleveland indians 3b w. t. white high school ( dallas, tx ) 12 jeff juden houston astros rhp salem ( ma ) hs 13 brent mayne kansas city royals c cal state fullerton 14 steve hosey san francisco giants of fresno state university 15 kiki jones los angeles dodgers rhp hillsborough hs ( tampa, fl ) 16 greg blosser boston red sox of sarasota ( fl ) hs 17 cal eldred milwaukee brewers rhp university of iowa 18 willie greene pittsburgh pirates ss jones county hs ( gray, ga ) 19 eddie zosky toronto blue jays ss fresno state university 20 scott bryant cincinnati reds of university of texas 21 greg gohr detroit tigers rhp santa clara university 22 tom goodwin los angeles dodgers of fresno state university 23 mo vaughn boston red sox 1b seton hall university 24 alan zinter new york mets c university of arizona 25 chuck knoblauch minnesota twins 2b texas a & m university 26 scott burrell seattle mariners rhp hamden ( ct ) hs [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6cb86fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(132)\n"
     ]
    }
   ],
   "source": [
    "assert batch[\"labels\"][0].sum() == batch[\"token_type_ids\"][1][:,3].sum()\n",
    "print(batch[\"token_type_ids\"][1][:,3].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72cefdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 0\n",
      "which 0\n",
      "player 0\n",
      "went 0\n",
      "to 0\n",
      "louisiana 0\n",
      "state 0\n",
      "university 0\n",
      "? 0\n",
      "[SEP] 0\n",
      "pick 0\n",
      "player 0\n",
      "team 0\n",
      "position 0\n",
      "school 0\n",
      "1 0\n",
      "ben 0\n",
      "mcdonald 0\n",
      "baltimore 0\n",
      "orioles 0\n",
      "r 0\n",
      "##hp 0\n",
      "louisiana 1\n",
      "state 1\n",
      "university 1\n",
      "2 0\n",
      "tyler 0\n",
      "houston 0\n",
      "atlanta 0\n",
      "braves 0\n",
      "c 0\n",
      "valley 1\n",
      "hs 1\n",
      "( 1\n",
      "las 1\n",
      "vegas 1\n",
      ", 1\n",
      "n 1\n",
      "##v 1\n",
      ") 1\n",
      "3 0\n",
      "roger 0\n",
      "sal 0\n",
      "##kel 0\n",
      "##d 0\n",
      "seattle 0\n",
      "mariners 0\n",
      "r 0\n",
      "##hp 0\n",
      "sa 1\n",
      "##ug 1\n",
      "##us 1\n",
      "( 1\n",
      "ca 1\n",
      ") 1\n",
      "hs 1\n",
      "4 0\n",
      "jeff 0\n",
      "jackson 0\n",
      "philadelphia 0\n",
      "phillies 0\n",
      "of 0\n",
      "simeon 1\n",
      "hs 1\n",
      "( 1\n",
      "chicago 1\n",
      ", 1\n",
      "il 1\n",
      ") 1\n",
      "5 0\n",
      "donald 0\n",
      "harris 0\n",
      "texas 0\n",
      "rangers 0\n",
      "of 0\n",
      "texas 1\n",
      "tech 1\n",
      "university 1\n",
      "6 0\n",
      "paul 0\n",
      "coleman 0\n",
      "saint 0\n",
      "louis 0\n",
      "cardinals 0\n",
      "of 0\n",
      "franks 1\n",
      "##ton 1\n",
      "( 1\n",
      "tx 1\n",
      ") 1\n",
      "hs 1\n",
      "7 0\n",
      "frank 0\n",
      "thomas 0\n",
      "chicago 0\n",
      "white 0\n",
      "sox 0\n",
      "1b 0\n",
      "auburn 1\n",
      "university 1\n",
      "8 0\n",
      "earl 0\n",
      "cunningham 0\n",
      "chicago 0\n",
      "cubs 0\n",
      "of 0\n",
      "lancaster 1\n",
      "( 1\n",
      "sc 1\n",
      ") 1\n",
      "hs 1\n",
      "9 0\n",
      "kyle 0\n",
      "abbott 0\n",
      "california 0\n",
      "angels 0\n",
      "l 0\n",
      "##hp 0\n",
      "long 1\n",
      "beach 1\n",
      "state 1\n",
      "university 1\n",
      "10 0\n",
      "charles 0\n",
      "johnson 0\n",
      "montreal 0\n",
      "expo 0\n",
      "##s 0\n",
      "c 0\n",
      "westwood 1\n",
      "hs 1\n",
      "( 1\n",
      "fort 1\n",
      "pierce 1\n",
      ", 1\n",
      "fl 1\n",
      ") 1\n",
      "11 0\n",
      "calvin 0\n",
      "murray 0\n",
      "cleveland 0\n",
      "indians 0\n",
      "3 0\n",
      "##b 0\n",
      "w 1\n",
      ". 1\n",
      "t 1\n",
      ". 1\n",
      "white 1\n",
      "high 1\n",
      "school 1\n",
      "( 1\n",
      "dallas 1\n",
      ", 1\n",
      "tx 1\n",
      ") 1\n",
      "12 0\n",
      "jeff 0\n",
      "jude 0\n",
      "##n 0\n",
      "houston 0\n",
      "astros 0\n",
      "r 0\n",
      "##hp 0\n",
      "salem 1\n",
      "( 1\n",
      "ma 1\n",
      ") 1\n",
      "hs 1\n",
      "13 0\n",
      "brent 0\n",
      "may 0\n",
      "##ne 0\n",
      "kansas 0\n",
      "city 0\n",
      "royals 0\n",
      "c 0\n",
      "cal 1\n",
      "state 1\n",
      "fuller 1\n",
      "##ton 1\n",
      "14 0\n",
      "steve 0\n",
      "hose 0\n",
      "##y 0\n",
      "san 0\n",
      "francisco 0\n",
      "giants 0\n",
      "of 0\n",
      "fresno 1\n",
      "state 1\n",
      "university 1\n",
      "15 0\n",
      "ki 0\n",
      "##ki 0\n",
      "jones 0\n",
      "los 0\n",
      "angeles 0\n",
      "dodgers 0\n",
      "r 0\n",
      "##hp 0\n",
      "hillsborough 1\n",
      "hs 1\n",
      "( 1\n",
      "tampa 1\n",
      ", 1\n",
      "fl 1\n",
      ") 1\n",
      "16 0\n",
      "greg 0\n",
      "b 0\n",
      "##los 0\n",
      "##ser 0\n",
      "boston 0\n",
      "red 0\n",
      "sox 0\n",
      "of 0\n",
      "sara 1\n",
      "##so 1\n",
      "##ta 1\n",
      "( 1\n",
      "fl 1\n",
      ") 1\n",
      "hs 1\n",
      "17 0\n",
      "cal 0\n",
      "el 0\n",
      "##dre 0\n",
      "##d 0\n",
      "milwaukee 0\n",
      "brewers 0\n",
      "r 0\n",
      "##hp 0\n",
      "university 1\n",
      "of 1\n",
      "iowa 1\n",
      "18 0\n",
      "willie 0\n",
      "greene 0\n",
      "pittsburgh 0\n",
      "pirates 0\n",
      "ss 0\n",
      "jones 1\n",
      "county 1\n",
      "hs 1\n",
      "( 1\n",
      "gray 1\n",
      ", 1\n",
      "ga 1\n",
      ") 1\n",
      "19 0\n",
      "eddie 0\n",
      "z 0\n",
      "##os 0\n",
      "##ky 0\n",
      "toronto 0\n",
      "blue 0\n",
      "jays 0\n",
      "ss 0\n",
      "fresno 1\n",
      "state 1\n",
      "university 1\n",
      "20 0\n",
      "scott 0\n",
      "bryant 0\n",
      "cincinnati 0\n",
      "reds 0\n",
      "of 0\n",
      "university 1\n",
      "of 1\n",
      "texas 1\n",
      "21 0\n",
      "greg 0\n",
      "go 0\n",
      "##hr 0\n",
      "detroit 0\n",
      "tigers 0\n",
      "r 0\n",
      "##hp 0\n",
      "santa 1\n",
      "clara 1\n",
      "university 1\n",
      "22 0\n",
      "tom 0\n",
      "goodwin 0\n",
      "los 0\n",
      "angeles 0\n",
      "dodgers 0\n",
      "of 0\n",
      "fresno 1\n",
      "state 1\n",
      "university 1\n",
      "23 0\n",
      "mo 0\n",
      "vaughn 0\n",
      "boston 0\n",
      "red 0\n",
      "sox 0\n",
      "1b 0\n",
      "seton 1\n",
      "hall 1\n",
      "university 1\n",
      "24 0\n",
      "alan 0\n",
      "z 0\n",
      "##int 0\n",
      "##er 0\n",
      "new 0\n",
      "york 0\n",
      "mets 0\n",
      "c 0\n",
      "university 1\n",
      "of 1\n",
      "arizona 1\n",
      "25 0\n",
      "chuck 0\n",
      "knob 0\n",
      "##lau 0\n",
      "##ch 0\n",
      "minnesota 0\n",
      "twins 0\n",
      "2 0\n",
      "##b 0\n",
      "texas 1\n",
      "a 1\n",
      "& 1\n",
      "m 1\n",
      "university 1\n",
      "26 0\n",
      "scott 0\n",
      "burr 0\n",
      "##ell 0\n",
      "seattle 0\n",
      "mariners 0\n",
      "r 0\n",
      "##hp 0\n",
      "ham 1\n",
      "##den 1\n",
      "( 1\n",
      "ct 1\n",
      ") 1\n",
      "hs 1\n"
     ]
    }
   ],
   "source": [
    "for id, prev_label in zip(batch[\"input_ids\"][1], batch[\"token_type_ids\"][1][:,3]):\n",
    "    if id != 0:\n",
    "        print(tokenizer.decode([id]), prev_label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c0819",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae22db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base and are newly initialized: ['column_output_weights', 'output_bias', 'output_weights', 'column_output_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TapasForQuestionAnswering(\n",
       "  (tapas): TapasModel(\n",
       "    (embeddings): TapasEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 768)\n",
       "      (token_type_embeddings_0): Embedding(3, 768)\n",
       "      (token_type_embeddings_1): Embedding(256, 768)\n",
       "      (token_type_embeddings_2): Embedding(256, 768)\n",
       "      (token_type_embeddings_3): Embedding(2, 768)\n",
       "      (token_type_embeddings_4): Embedding(256, 768)\n",
       "      (token_type_embeddings_5): Embedding(256, 768)\n",
       "      (token_type_embeddings_6): Embedding(10, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.07, inplace=False)\n",
       "    )\n",
       "    (encoder): TapasEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.07, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.07, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): TapasPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.07, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TapasForQuestionAnswering\n",
    "\n",
    "model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1443d1",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c531cedb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 14.76 GiB total capacity; 8.25 GiB already allocated; 9.75 MiB free; 8.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m     18\u001b[0m                labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1229\u001b[0m, in \u001b[0;36mTapasForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, table_mask, labels, aggregation_labels, float_answer, numeric_values, numeric_values_scale, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;124;03mtable_mask (`torch.LongTensor` of shape `(batch_size, seq_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;124;03m    Mask for the table. Indicates which tokens belong to the table (1). Question tokens, table headers and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;124;03m>>> logits_aggregation = outputs.logits_aggregation\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtapas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1242\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:981\u001b[0m, in \u001b[0;36mTapasModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    976\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    978\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    979\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds\n\u001b[1;32m    980\u001b[0m )\n\u001b[0;32m--> 981\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    992\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:670\u001b[0m, in \u001b[0;36mTapasEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    661\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    662\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    663\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    667\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:565\u001b[0m, in \u001b[0;36mTapasLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    555\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:492\u001b[0m, in \u001b[0;36mTapasAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    484\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    491\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 492\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    502\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ssd003/projects/aieng/conversational_ai/envs/stable_env/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:411\u001b[0m, in \u001b[0;36mTapasSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    408\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in TapasModel forward() function)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 14.76 GiB total capacity; 8.25 GiB already allocated; 9.75 MiB free; 8.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        # get the inputs;\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                       labels=labels)\n",
    "        loss = outputs.loss\n",
    "        print(\"Loss:\", loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9362ddd",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47733490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def compute_prediction_sequence(model, data, device):\n",
    "    \"\"\"Computes predictions using model's answers to the previous questions.\"\"\"\n",
    "  \n",
    "    # prepare data\n",
    "    input_ids = data[\"input_ids\"].to(device)\n",
    "    attention_mask = data[\"attention_mask\"].to(device)\n",
    "    token_type_ids = data[\"token_type_ids\"].to(device)\n",
    "\n",
    "    all_logits = []\n",
    "    prev_answers = None\n",
    "\n",
    "    num_batch = data[\"input_ids\"].shape[0]\n",
    "\n",
    "    for idx in range(num_batch):\n",
    "        if prev_answers is not None:\n",
    "            coords_to_answer = prev_answers[idx]\n",
    "            # Next, set the label ids predicted by the model\n",
    "            prev_label_ids_example = token_type_ids_example[:,3] # shape (seq_len,)\n",
    "            model_label_ids = np.zeros_like(prev_label_ids_example.cpu().numpy()) # shape (seq_len,)\n",
    "\n",
    "            # for each token in the sequence:\n",
    "            token_type_ids_example = token_type_ids[idx] # shape (seq_len, 7)\n",
    "            for i in range(model_label_ids.shape[0]):\n",
    "                segment_id = token_type_ids_example[:,0].tolist()[i]\n",
    "                col_id = token_type_ids_example[:,1].tolist()[i] - 1\n",
    "                row_id = token_type_ids_example[:,2].tolist()[i] - 1\n",
    "                if row_id >= 0 and col_id >= 0 and segment_id == 1:\n",
    "                    model_label_ids[i] = int(coords_to_answer[(col_id, row_id)])\n",
    "\n",
    "            # set the prev label ids of the example (shape (1, seq_len) )\n",
    "            token_type_ids_example[:,3] = torch.from_numpy(model_label_ids).type(torch.long).to(device)   \n",
    "\n",
    "    prev_answers = {}\n",
    "    # get the example\n",
    "    input_ids_example = input_ids[idx] # shape (seq_len,)\n",
    "    attention_mask_example = attention_mask[idx] # shape (seq_len,)\n",
    "    token_type_ids_example = token_type_ids[idx] # shape (seq_len, 7)\n",
    "    # forward pass to obtain the logits\n",
    "    outputs = model(input_ids=input_ids_example.unsqueeze(0), \n",
    "                    attention_mask=attention_mask_example.unsqueeze(0), \n",
    "                    token_type_ids=token_type_ids_example.unsqueeze(0))\n",
    "    logits = outputs.logits\n",
    "    all_logits.append(logits)\n",
    "\n",
    "    # convert logits to probabilities (which are of shape (1, seq_len))\n",
    "    dist_per_token = torch.distributions.Bernoulli(logits=logits)\n",
    "    probabilities = dist_per_token.probs * attention_mask_example.type(torch.float32).to(dist_per_token.probs.device) \n",
    "\n",
    "    # Compute average probability per cell, aggregating over tokens.\n",
    "    # Dictionary maps coordinates to a list of one or more probabilities\n",
    "    coords_to_probs = collections.defaultdict(list)\n",
    "    prev_answers = {}\n",
    "    for i, p in enumerate(probabilities.squeeze().tolist()):\n",
    "        segment_id = token_type_ids_example[:,0].tolist()[i]\n",
    "        col = token_type_ids_example[:,1].tolist()[i] - 1\n",
    "        row = token_type_ids_example[:,2].tolist()[i] - 1\n",
    "        if col >= 0 and row >= 0 and segment_id == 1:\n",
    "            coords_to_probs[(col, row)].append(p)\n",
    "\n",
    "    # Next, map cell coordinates to 1 or 0 (depending on whether the mean prob of all cell tokens is > 0.5)\n",
    "    coords_to_answer = {}\n",
    "    for key in coords_to_probs:\n",
    "        coords_to_answer[key] = np.array(coords_to_probs[key]).mean() > 0.5\n",
    "    prev_answers[idx+1] = coords_to_answer\n",
    "\n",
    "    logits_batch = torch.cat(tuple(all_logits), 0)\n",
    "\n",
    "    return logits_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f53d72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Actors': [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \n",
    "        'Age': [\"56\", \"45\", \"59\"],\n",
    "        'Number of movies': [\"87\", \"53\", \"69\"],\n",
    "        'Date of birth': [\"7 february 1967\", \"10 june 1996\", \"28 november 1967\"]}\n",
    "queries = [\"How many movies has George Clooney played in?\", \"How old is he?\", \"What's his date of birth?\"]\n",
    "\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "inputs = tokenizer(table=table, queries=queries, padding='max_length', return_tensors=\"pt\")\n",
    "logits = compute_prediction_sequence(model, inputs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b347a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_answer_coordinates, = tokenizer.convert_logits_to_predictions(inputs, logits.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c0e3b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actors</th>\n",
       "      <th>Age</th>\n",
       "      <th>Number of movies</th>\n",
       "      <th>Date of birth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brad Pitt</td>\n",
       "      <td>56</td>\n",
       "      <td>87</td>\n",
       "      <td>7 february 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Leonardo Di Caprio</td>\n",
       "      <td>45</td>\n",
       "      <td>53</td>\n",
       "      <td>10 june 1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George Clooney</td>\n",
       "      <td>59</td>\n",
       "      <td>69</td>\n",
       "      <td>28 november 1967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Actors Age Number of movies     Date of birth\n",
       "0           Brad Pitt  56               87   7 february 1967\n",
       "1  Leonardo Di Caprio  45               53      10 june 1996\n",
       "2      George Clooney  59               69  28 november 1967"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "How many movies has George Clooney played in?\n",
      "Predicted answer: 56, 87, 45, 53, 59, 69\n",
      "How old is he?\n",
      "Predicted answer: Leonardo Di Caprio, George Clooney\n",
      "What's his date of birth?\n",
      "Predicted answer: 7 february 1967, 10 june 1996, 28 november 1967\n"
     ]
    }
   ],
   "source": [
    "# handy helper function in case inference on Pandas dataframe\n",
    "answers = []\n",
    "for coordinates in predicted_answer_coordinates:\n",
    "    if len(coordinates) == 1:\n",
    "    # only a single cell:\n",
    "        answers.append(table.iat[coordinates[0]])\n",
    "    else:\n",
    "        # multiple cells\n",
    "        cell_values = []\n",
    "    for coordinate in coordinates:\n",
    "        cell_values.append(table.iat[coordinate])\n",
    "    answers.append(\", \".join(cell_values))\n",
    "\n",
    "display(table)\n",
    "print(\"\")\n",
    "for query, answer in zip(queries, answers):\n",
    "    print(query)\n",
    "    print(\"Predicted answer: \" + answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41a51f7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa3c5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('sqa_data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b89fe3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>annotator</th>\n",
       "      <th>position</th>\n",
       "      <th>question</th>\n",
       "      <th>table_file</th>\n",
       "      <th>answer_coordinates</th>\n",
       "      <th>answer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nu-597</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>where did the championships take place?</td>\n",
       "      <td>table_csv/204_758.csv</td>\n",
       "      <td>['(0, 3)', '(1, 3)', '(2, 3)', '(3, 3)', '(4, ...</td>\n",
       "      <td>['Memphis, Tennessee, USA', 'Coral Springs, Fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nu-597</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>and on what dates?</td>\n",
       "      <td>table_csv/204_758.csv</td>\n",
       "      <td>['(0, 2)', '(1, 2)', '(2, 2)', '(3, 2)', '(4, ...</td>\n",
       "      <td>['February 15, 1993', 'May 17, 1993', 'July 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nu-597</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>what about just atlanta, georgia, usa?</td>\n",
       "      <td>table_csv/204_758.csv</td>\n",
       "      <td>['(7, 2)']</td>\n",
       "      <td>['May 2, 1994']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nu-597</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>now, which other event took place in that month?</td>\n",
       "      <td>table_csv/204_758.csv</td>\n",
       "      <td>['(8, 3)']</td>\n",
       "      <td>['Pinehurst, USA']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nu-597</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>what are the location names in the championship?</td>\n",
       "      <td>table_csv/204_758.csv</td>\n",
       "      <td>['(0, 3)', '(1, 3)', '(2, 3)', '(3, 3)', '(4, ...</td>\n",
       "      <td>['Memphis, Tennessee, USA', 'Coral Springs, Fl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  annotator  position  \\\n",
       "0  nu-597          0         0   \n",
       "1  nu-597          0         1   \n",
       "2  nu-597          0         2   \n",
       "3  nu-597          0         3   \n",
       "4  nu-597          1         0   \n",
       "\n",
       "                                           question             table_file  \\\n",
       "0           where did the championships take place?  table_csv/204_758.csv   \n",
       "1                                and on what dates?  table_csv/204_758.csv   \n",
       "2            what about just atlanta, georgia, usa?  table_csv/204_758.csv   \n",
       "3  now, which other event took place in that month?  table_csv/204_758.csv   \n",
       "4  what are the location names in the championship?  table_csv/204_758.csv   \n",
       "\n",
       "                                  answer_coordinates  \\\n",
       "0  ['(0, 3)', '(1, 3)', '(2, 3)', '(3, 3)', '(4, ...   \n",
       "1  ['(0, 2)', '(1, 2)', '(2, 2)', '(3, 2)', '(4, ...   \n",
       "2                                         ['(7, 2)']   \n",
       "3                                         ['(8, 3)']   \n",
       "4  ['(0, 3)', '(1, 3)', '(2, 3)', '(3, 3)', '(4, ...   \n",
       "\n",
       "                                         answer_text  \n",
       "0  ['Memphis, Tennessee, USA', 'Coral Springs, Fl...  \n",
       "1  ['February 15, 1993', 'May 17, 1993', 'July 26...  \n",
       "2                                    ['May 2, 1994']  \n",
       "3                                 ['Pinehurst, USA']  \n",
       "4  ['Memphis, Tennessee, USA', 'Coral Springs, Fl...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5b8b52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'annotator', 'position', 'question', 'table_file',\n",
       "       'answer_coordinates', 'answer_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b39c8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_test = test_data.loc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa27c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TableDataset(df=mini_test, tokenizer=tokenizer)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dcb46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
